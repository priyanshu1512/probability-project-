# -*- coding: utf-8 -*-
"""loan_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wkv9VUQswdte6_2UZv2EWz4v-pmqIOIm
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("LoanApprovalPrediction.csv")                                    #importing dataset

data.head(5)                                                                      #view first 5 rows

obj = (data.dtypes == 'object')                                                   #data preprocessing and visualising
print("Categorical variables:",len(list(obj[obj].index)))

# Dropping Loan_ID column because it is unique and not correlated with anyother column
data.drop(['Loan_ID'],axis=1,inplace=True)

obj = (data.dtypes == 'object')
object_cols = list(obj[obj].index)                                                  #find columsn of data type object and them stores count in obj_cols
plt.figure(figsize=(18,36))
index = 1

for col in object_cols:
  y = data[col].value_counts()                                                         #counts frequency of unique elements in column
  plt.subplot(11,4,index)
  plt.xticks(rotation=90)
  sns.barplot(x=list(y.index), y=y)                                                    #barplots for all unique values stored in x and y telling their frequecny
  index +=1

# Import label encoder
from sklearn import preprocessing

# label_encoder object knows how
# to understand word labels.                                                      #transform the object data type columns to numerical values
label_encoder = preprocessing.LabelEncoder()
obj = (data.dtypes == 'object')
for col in list(obj[obj].index):
  data[col] = label_encoder.fit_transform(data[col])

# To find the number of columns with
# datatype==object
obj = (data.dtypes == 'object')
print("Categorical variables:",len(list(obj[obj].index)))

plt.figure(figsize=(12,6))

sns.heatmap(data.corr(),cmap='BrBG',fmt='.2f',                                 		#constructing heatmap
			linewidths=2,annot=True)
#The above heatmap is showing the correlation between Loan Amount and ApplicantIncome. It also shows that Credit_History has a high impact on Loan_Status.

sns.catplot(x="Gender", y="Married",
			hue="Loan_Status",
			kind="bar",
			data=data)

for col in data.columns:
  data[col] = data[col].fillna(data[col].mean())                                       #calculating if any NaN values are left

data.isna().sum()

from sklearn.model_selection import train_test_split
																																																							#splitting dataset for model training
X = data.drop(['Loan_Status'],axis=1)
Y = data['Loan_Status']
X.shape,Y.shape

X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
													test_size=0.4,
													random_state=1)
X_train.shape, X_test.shape, Y_train.shape, Y_test.shape

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC                                                      #support vector machines # points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible.
from sklearn.linear_model import LogisticRegression															 #It is used for predicting the categorical dependent variable using a given set of independent variables.

from sklearn import metrics

knn = KNeighborsClassifier(n_neighbors=3)
rfc = RandomForestClassifier(n_estimators = 7,
							criterion = 'entropy',
							random_state =7)
svc = SVC()
lc = LogisticRegression()

# making predictions on the training set
for clf in (rfc, knn, svc,lc):
	clf.fit(X_train, Y_train)
	Y_pred = clf.predict(X_train)
	print("Accuracy score of ",
		clf.__class__.__name__,
		"=",100*metrics.accuracy_score(Y_train,
										Y_pred))

# making predictions on the testing set
for clf in (rfc, knn, svc,lc):
	clf.fit(X_train, Y_train)
	Y_pred = clf.predict(X_test)
	print("Accuracy score of ",
		clf.__class__.__name__,"=",
		100*metrics.accuracy_score(Y_test,
									Y_pred))

#Conclusion :
#Random Forest Classifier is giving the best accuracy with an accuracy score of 82% for the testing dataset.